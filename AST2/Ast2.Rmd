---
title: "DASC6510 Assignment2"
author: "Hao Xu T00732492"
output:
  html_document:
    css: style.css
  pdf_document:
    includes: 
      in_header: preamble.tex
---

Source code of this document: [https://github.com/cld4h/DASC_6510_Assignment/tree/master/AST2](https://github.com/cld4h/DASC_6510_Assignment/tree/master/AST2)

# Problem 1: Exercise 8.3 (page 242)

\prob{
Hierarchical modeling: The files school1.dat through school8.dat give weekly hours spent on homework for students sampled from eight different schools. Obtain posterior distributions for the true means for the eight different schools using a hierarchical normal model with the following prior parameters:
(Refer to Page 132)

\[
\mu_0 = 7, \gamma_0^2 = 5, \quad \tau_0^2=10, \eta_0=2, \quad \sigma_0^2 = 15, \nu_0=2.
\] 
}

## Part a

\prob{
a) Run a Gibbs sampling algorithm to approximate the posterior distribution of $\{\boldsymbol{\theta}, \sigma^{2}, \mu, \tau^{2}\}$. Assess the convergence of the Markov chain, and find the effective sample size for $\{\sigma^2, \mu, \tau^2\}$. Run the chain long enough so that the effective sample sizes are all above 1,000.
}
\begin{figure}
    \centering
\begin{tikzpicture}
\matrix[matrix of math nodes, column sep=20pt, row sep=20pt] (mat)
{
    &&\left(\mu, \tau^2\right) && \\ 
    \theta_1 & \theta_2 & \ldots & \theta_{m-1} & \theta_m \\
    \mathbf{Y}_1 & \mathbf{Y}_2 & \ldots & \mathbf{Y}_{m-1} &  \mathbf{Y}_m \\
    && \sigma^2 && \\
};

\foreach \column in {1, 2, 4, 5}
{
    \draw[->,>=latex] (mat-1-3) -- (mat-2-\column);
    \draw[->,>=latex] (mat-2-\column) -- (mat-3-\column);
    \draw[<-,>=latex] (mat-3-\column) -- (mat-4-3);
}
\end{tikzpicture}
\caption{Graphical representation of the hierarchical model.}
\label{fig:8.3diagram}
\end{figure}
Because here only $\boldsymbol{\theta}$ is a vector ($\boldsymbol{\theta} = (\theta_1,\ldots,\theta_m)^T$, $m = 8$ is the total number of schools), $\sigma^2$ is the same across all 8 schools, so we are assuming common variance. The fixed but unknown parameters in this model are $\mu, \tau^2$ and $\sigma^2$. We use standard semiconjugate normal and inverse-gamma prior distributions for these parameters:

\begin{align*}
	\frac{1}{\sigma^2} &\sim \text{gamma}(\frac{\nu_0}{2}, \frac{\nu_0\sigma_0^2}{2})\\
	\frac{1}{\tau^2} &\sim \text{gamma}(\frac{\eta_0}{2}, \frac{\eta_0\tau_0^2}{2})\\
	\mu &\sim \text{normal}(\mu_0, \gamma_0^2)\\
\end{align*}

Posterior approximation proceeds by iterative sampling of each unknown quantity from its full conditional distribution. Given a current state of the unknowns $\{\theta_1^{(s)},\ldots,\theta_m^{(s)}, \mu^{(s)}, {\tau^2}^{(s)},{\sigma^2}^{(s)}\}$, a new state is generated as follows:

1. sample $\mu^{(s+1)}\sim p(\mu|\theta_1^{(s)},\ldots,\theta_m^{(s)}, {\tau^2}^{(s)})$
2. sample ${\tau^2}^{(s+1)} \sim p(\tau^2|\theta_1^{(s)},\ldots,\theta_m^{(s)}, \mu^{(s+1)})$
3. sample ${\sigma^2}^{(s+1)} \sim p(\sigma^2|\theta_1^{(s)},\ldots,\theta_m^{(s)}, \mathbf{Y}_1,\ldots,\mathbf{Y}_m)$
4. for each $j \in \{1,\ldots,m\}$, sample $\theta_j^{(s+1)}\sim p(\theta_j|\mu^{(s+1)}, {\tau^2}^{(s+1)},{\sigma^2}^{(s+1)}, \mathbf{Y}_j)$
\begin{align*}
	\{ \mu|\theta_1, \ldots, \theta_m, \tau^2\} &\sim \text{normal}\left( \frac{m \overline{\theta}/\tau^2+\mu_0/\gamma_0^2}{m/\tau^2+1/\gamma_0^2}, [m/\tau^2+1/\gamma_0^2]^{-1} \right) \\
	\{ \frac{1}{\tau^2}|\theta_1, \ldots, \theta_m, \mu\} &\sim \text{gamma}\left( \frac{\eta_0+m}{2}, \frac{\eta_0\tau_0^2+\sum(\theta_j-\mu)^2}{2} \right) \\
	\{\theta_j|Y_{1,j},\ldots,Y_{n_j, j}, \sigma^2\} &\sim \text{normal}\left(\frac{n_j \overline{Y_{\cdot,j}} / \sigma^2 + 1 / \tau^2}{n_j / \sigma^2 + 1/\tau^2},[n_j / \sigma^2 + 1/\tau^2]^{-1}\right)\\
	\{ \frac{1}{\sigma^2}|\theta_1, \ldots, \theta_m, Y_1,\ldots,Y_n\} &\sim \text{gamma}\left( \frac{1}{2}\left[\nu_0+\sum_{j=1}^{m} n_j\right], \frac{1}{2}\left[\nu_0\sigma_0^2+\sum_{j=1}^{m} \sum_{i=1}^{n_j} (Y_{i,j}-\theta_j)^2\right] \right) \\
\end{align*}

```{r}
### Code modified from Ch.8.4.1
### weakly informative priors
mu0 <- 7; g20 <- 5
t20 <- 10; eta0 <- 2
s20 <- 15; nu0 <- 2
###

Y <- NULL
### load the data
# iterate from 1 to 8
for (i in 1:8) {
# store schooli.dat into Y
scores <- scan(paste0("school",i,".dat")) 
      for(j in 1:length(scores)){
		Y <- rbind(Y, c(i,scores[j]))
      }
}

## starting values
m<-length(unique(Y[,1])) # the number of schools, m=8
n<-sv<-ybar<-rep(NA,m) 
for(j in 1:m) 
{ 
  # mean score for each school
  ybar[j]<-mean(Y[Y[,1]==j,2]) 
  # score variance for each school
  sv[j]<-var(Y[Y[,1]==j,2])
  # number of students in each school
  n[j]<-sum(Y[,1]==j) 
}
# initial value?
theta<-ybar; sigma2<-mean(sv)
mu<-mean(theta); tau2<-var(theta)
###

## setup MCMC
set.seed(1)
S<-5000
THETA<-matrix( nrow=S,ncol=m)
MST<-matrix( nrow=S,ncol=3)

## MCMC algorithm
for(s in 1:S) 
{

  # sample new values of the thetas
  for(j in 1:m) 
  {
    # Variance of full conditional distribution of
    # theta given y_{1,j}..y_{n_j,j} and \sigma^2
    vtheta<-1/(n[j]/sigma2+1/tau2)
    # Mean of full conditional distribution of
    # theta given y_{1,j}..y_{n_j,j} and \sigma^2
    etheta<-vtheta*(ybar[j]*n[j]/sigma2+mu/tau2)
    theta[j]<-rnorm(1,etheta,sqrt(vtheta))
   }

  # sample new value of sigma2 from inverse-gamma
  nun<-nu0+sum(n)
  ss<-nu0*s20
  for(j in 1:m){ss<-ss+sum((Y[Y[,1]==j,2]-theta[j])^2)}
  sigma2<-1/rgamma(1,nun/2,ss/2)

  #sample a new value of mu from normal
  vmu<- 1/(m/tau2+1/g20)
  emu<- vmu*(m*mean(theta)/tau2 + mu0/g20)
  mu<-rnorm(1,emu,sqrt(vmu)) 

  # sample a new value of tau2 from gamma
  etam<-eta0+m
  ss<- eta0*t20 + sum( (theta-mu)^2 )
  tau2<-1/rgamma(1,etam/2,ss/2)

  #store results
  THETA[s,]<-theta
  MST[s,]<-c(mu,sigma2,tau2)
} 
```

Assess the convergence of the Markov chain
```{r}
#### A plot for evaluating lack of convergence
stationarity.plot<-function(x,...){

S<-length(x)
scan<-1:S
ng<-min( round(S/100),10)
group<-S*ceiling( ng*scan/S) /ng

boxplot(x~group,...)               
}
stationarity.plot(MST[,1],xlab="iteration",ylab=expression(mu))
stationarity.plot(MST[,2],xlab="iteration",ylab=expression(sigma^2))
stationarity.plot(MST[,3],xlab="iteration",ylab=expression(tau^2))
```

\prob{
Find the effective sample size for $\{\mu, \sigma^2, \tau^2\}$.
}

```{r}
library(coda)
# The effective size for mu is: 
effectiveSize(MST[, 1])
# The effective size for sigma^2 is: 
effectiveSize(MST[, 2])
# The effective size for tau^2 is:
effectiveSize(MST[, 3])
```

## Part b

\prob{
b) Compute posterior means and 95\% confidence regions for $\{\sigma^2, \mu, \tau^2\}$. Also, compare the posterior densities to the prior densities, and discuss what was learned from the data.
}

```{r}
B <- 201 # Burn-in first 200 samples
library(xtable)

Conf.Regions <- t(apply(MST[B:S,], MARGIN = 2, FUN = quantile, probs = c(0.025, 0.5, 0.975)))
Post.Mean <- t(t(apply(MST[B:S,], MARGIN = 2, FUN = mean)))
cbind(Post.Mean,Conf.Regions)
# xtable( cbind(Post.Mean,Conf.Regions))
```

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Posterior Mean& 2.5\% & 50\% & 97.5\% \\ 
  \hline
$\mu$ & 7.55 & 5.92 & 7.57 & 9.13 \\ 
$\sigma^2$ & 14.48 & 11.73 & 14.34 & 17.79 \\ 
$\tau^2$ & 5.61 & 1.90 & 4.63 & 14.97 \\ 
   \hline
\end{tabular}
\end{table}

Comparing posterior densities to the prior densities:

```{r}
plot(density(MST[B:S,1]), main = "Posterior density(black solid) vs Prior density(gray dashed)", 
     xlab = expression(mu), ylab = expression(paste("p(", mu, ")", sep="")))
x_mu <- seq(min(MST[B:S,1]), max(MST[B:S,1]), length.out = S)
lines(x_mu,dnorm(x_mu,mean=mu0,sd=sqrt(g20)),type="l",col="gray",lwd=2,lty=2)

dinvgamma<-function(x,a,b) {
ld<- a*log(b) -lgamma(a) -(a+1)*log(x)  -b/x
exp(ld)
}

plot(density(MST[B:S,2]), main = "Posterior density(black solid) vs Prior density(gray dashed)", 
     xlab = expression(sigma^2), ylab = expression(paste("p(", sigma^2, ")", sep="")))
x_sigma <- seq(min(MST[B:S,2]), max(MST[B:S,2]), length.out = S)
lines(x_sigma,dinvgamma(x_sigma,a=nu0/2,b=(nu0*s20)/2),type="l",col="gray",lwd=2,lty=2)

plot(density(MST[B:S,3]), main = "Posterior density(black solid) vs Prior density(gray dashed)", 
     xlab = expression(tau^2), ylab = expression(paste("p(", tau^2, ")", sep="")))
x_tau <- seq(min(MST[B:S,3]), max(MST[B:S,3]), length.out = S)
lines(x_tau,dinvgamma(x_sigma,a=eta0/2,b=(eta0*t20)/2),type="l",col="gray",lwd=2,lty=2)
```

The posterior distributions are more concentrated, having less variance compared to prior.

## Part c

\prob{
c) Plot the posterior density of $R = \frac{\tau^2}{\sigma^2+\tau^2}$ and compare it to a plot of the prior density of R. Describe the evidence for between-school variation.
}

```{r}
t20_sample = (1 / rgamma(1000000, eta0 / 2, eta0 * t20 / 2))
s20_sample = (1 / rgamma(1000000, nu0 / 2, nu0 * s20 / 2))

R0_sample =  (t20_sample) / (s20_sample+ t20_sample)

R_sample = MST[B:S, 3] / (MST[B:S, 2] + MST[B:S, 3])

plot(density(R_sample), main = "Posterior density(black solid) vs Prior density(gray dashed)", 
     xlab = expression(R), ylab = expression(paste("p(R)")))
lines(density(R0_sample), col="Gray", lwd=2, lty=2)
```

If $\sigma^2 \gg \tau^2$, $R\to 0$, the within-school variance is the dominating factor for the variance in data; If $\tau^2 \gg \sigma^2$, $R\to 1$,  the between-school variance is the dominating factor for the variance in data. $R$ indicate the proportion of between-school variance in the total variance of data. We have a weak prior on which part is dominating before inference, but after calculating the posterior distribution of $R$, we know that around 20\% of the variance in data comes from the between-school variance.


## Part d

\prob{
d) Obtain the posterior probability that $\theta_7$ is smaller than $\theta_6$, as well as the posterior probability that $\theta_7$ is the smallest of all the $\theta$â€™s.
}

```{r}
# The posterior probability that theta_7 is smaller than theta_6
theta7_lt_6 <- THETA[B:S, 7] < THETA[B:S, 6]
mean(theta7_lt_6)

# The posterior probability that theta_7 is smaller than all
theta7_lt_all <- THETA[B:S, 7] <= THETA[B:S, -7]
theta7_lt_all <- apply(theta7_lt_all, MARGIN=1, FUN=all)
mean(theta7_lt_all)
```

\begin{align*}
\text{Posterior }P(\theta_7 < \theta_6) &= 50.85\%\\
\text{Posterior }P(\theta_7 \leq \theta_\cdot) &= 30.58\%
\end{align*}

## Part e

\prob{
e) Plot the sample averages $\overline{y}_1,\ldots, \overline{y}_8$ against the posterior expectations of $\theta_1,\ldots,\theta_8$, and describe the relationship. Also compute the sample mean of all observations and compare it to the posterior mean of $\mu$.
}

```{r}
thetabar <- apply(THETA[B:S,], MARGIN=2, FUN=mean)
plot(thetabar,ybar, xlab=expression(theta[i]), 
ylab=expression(bar(Y[i])))
abline(lm(ybar~thetabar), lty=2)
# The sample mean of all observations
mean(Y[,2])
# Posterior mean of mu
mean(MST[B:S,1])
```

There is a linear relationship between $\theta_i$ and  $\overline{y_i}$. The sample mean of all observations is 7.69, The posterior mean of $\mu$ is 7.55.


# Problem 2: Exercise 9.3

\prob{
Crime: The file crime.dat contains crime rates and data on 15 explanatory variables for 47 U.S. states, in which both the crime rates and the explanatory variables have been centered and scaled to have variance 1. A description of the variables can be obtained by typing library(MASS);?UScrime in R.
}

## Part a
\prob{
a) Fit a regression model $\mathbf{y} = \mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$ using the $g$-prior with $g = n, \nu_0 = 2$ and $\sigma_0^2 = 1$. Obtain marginal posterior means and 95\% confidence intervals for $\boldsymbol{\beta}$, and compare to the least squares estimates. Describe the relationships between crime and the explanatory variables. Which variables seem strongly predictive of crime rates?
}

From textbook page 158, under $g$-prior distributionn, $p(\sigma^2|\mathbf{y,X})$ and $p(\boldsymbol{\beta}|\mathbf{y,X},\sigma^2)$ are inverse-gamma and multivariate normal distributions respectively. A sample value of $\sigma^2, \boldsymbol{\beta}$ from the joint posterior distributions $p(\sigma^2, \boldsymbol{\beta}|\mathbf{y,X})$ can be made with Monte Carlo approximation as follows:

\begin{enumerate}
\item sample $\frac{1}{\sigma^2} \sim \text{gamma}\left( [\nu_0+n]/2, [\nu_0\sigma_0^2+\text{SSR}_g]/2 \right) $ ; where $\text{SSR}_{g} = \mathbf{y}^T(\mathbf{I}-\frac{g}{g+1}\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T)\mathbf{y}$
\item sample $\boldsymbol{\beta} \sim \text{multivariate normal}\left( \frac{g}{g+1}\hat{\boldsymbol{\beta}}_\text{ols}, \frac{g}{g+1} \sigma^2[\mathbf{X}^T\mathbf{X}]^{-1} \right) $
\end{enumerate}


```{r}
crime <- read.table("crime.dat", header=TRUE)
# crime.df <- as.data.frame(read.table("crime.dat", header=TRUE))

y <- crime$y
X <- as.matrix(subset(crime, select = -c(y)))

n <- dim(X)[1] # rows of data
p <- dim(X)[2] # number of parameters

g = n
nu0 = 2
s20 = 1

S = 1000

Hg <- (g / (g + 1)) * X %*% solve(t(X) %*% X) %*% t(X)
SSRg <- t(y) %*% (diag(1, nrow = n) - Hg) %*% y

s2 <- 1 / rgamma(S, (nu0 + n) / 2, (nu0 * s20 + SSRg) / 2)
Vb <- g * solve(t(X) %*% X) / (g + 1)
Eb <- Vb %*% t(X) %*% y

E <- matrix(rnorm(S * p, 0, sqrt(s2)), S, p)
beta <- t(t(E %*% chol(Vb)) + c(Eb))

```

```{r}
CIbeta <- apply(beta, MARGIN=2, quantile, prob=c(0.025,0.5,0.975))
Significant <- apply(CIbeta, MARGIN=2, 
	function(q){return(ifelse(q[1]<0 & q[3]>0, FALSE, TRUE))})
Meanbeta <- apply(beta, MARGIN=2, mean)
LSbeta <- lm(y~X)$coeff[-1]
CIbeta <- rbind(CIbeta, Significant, Meanbeta, LSbeta)
# xtable(CIbeta)
plot(1:p,type="n", ylab=expression(beta[i]), xlab="index: i", 
     xlim=c(0,p+1), ylim=c(-3,3), xaxt="n")
for (i in 1:p){
color=ifelse(CIbeta[4,i],"black","gray")
segments(i,CIbeta[1,i],i,CIbeta[3,i], lwd=2,  col=color)
#points(i, CIbeta[1, i], pch=4, col=color)
points(i, CIbeta[2, i], col=color)
#points(i, CIbeta[3, i], pch=4, col=color)
}
axis(side = 1, at = seq(1,p,by=1), labels = colnames(CIbeta))
abline(h=0, lty=2, col="GRAY")
```

\begin{table}[ht]
\centering
\scalebox{0.8}{
\begin{tabular}{rrrrrrrrrrrrrrrr}
  \hline
 & M & So & Ed & Po1 & Po2 & LF & M.F & Pop & NW & U1 & U2 & GDP & Ineq 
& Prob & Time \\ 
  \hline
2.5\% & 0.05 & -0.34 & 0.21 & -0.02 & -2.20 & -0.31 & -0.16 & -0.29 & -0
.18 & -0.59 & 0.03 & -0.22 & 0.31 & -0.53 & -0.29 \\ 
  50\% & 0.28 & -0.01 & 0.54 & 1.42 & -0.77 & -0.07 & 0.13 & -0.06 & 0.1
1 & -0.26 & 0.35 & 0.22 & 0.70 & -0.27 & -0.06 \\ 
  97.5\% & 0.51 & 0.35 & 0.84 & 2.76 & 0.72 & 0.22 & 0.41 & 0.18 & 0.40 
& 0.09 & 0.65 & 0.68 & 1.12 & -0.03 & 0.20 \\ 
  Significant & 1.00 & 0.00 & 1.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 
0.00 & 0.00 & 1.00 & 0.00 & 1.00 & 1.00 & 0.00 \\ 
  Meanbeta & 0.28 & 0.00 & 0.53 & 1.42 & -0.74 & -0.06 & 0.13 & -0.07 & 
0.11 & -0.26 & 0.35 & 0.23 & 0.70 & -0.28 & -0.06 \\ 
  LSbeta & 0.29 & -0.00 & 0.54 & 1.47 & -0.78 & -0.07 & 0.13 & -0.07 & 0
.11 & -0.27 & 0.37 & 0.24 & 0.73 & -0.29 & -0.06 \\ 
   \hline
\end{tabular}
}
\end{table}

In the table above, 2.5\% and 97.5\% rows gives the lower and upper bound of 95\% credible intervals, Significant = 1 indicates variables are significant (does not include 0 in the credible intervals). The significant variables selected are: M, ED, U2, Ineq, Prob. "Meanbeta" row gives the marginal posterior means. The last row "LSbeta" gives the classic Least Square Regression result.

\prob{
b) Lets see how well regression models can predict crime rates based on the $\mathbf{X}$-variables. Randomly divide the crime roughly in half, into a training set $\{\mathbf{y}_{\text{tr}}, \mathbf{X}_{\text{tr}}\}$ and a test set $\{\mathbf{y}_{\text{te}}, \mathbf{X}_{\text{te}}\}$
}

```{r}
set.seed(1) 
train_i = sample.int(length(y), size = round(length(y) / 2), replace = FALSE)
ytr = y[train_i]
Xtr = X[train_i, ]
yte = y[-train_i]
Xte = X[-train_i, ]
```

\prob{
i. Using only the training set, obtain least squares regression coefficients $\hat{\boldsymbol{\beta}_{\text{ols}}}$. 
Obtain predicted values for the test data by computing $\hat{\mathbf{y}_{\text{ols}}}$. 
Plot $\hat{\mathbf{y}_{\text{ols}}}$ versus $\mathbf{y}_{\text{te}}$ 
and compute the prediction error 
$\frac{1}{n_{\text{te}}}\sum \left( y_{i\text{,te}} - \hat{y}_{i\text{,ols}} \right)^2$
}

The dashed grey line is the reference line for $y_{te}=y_{ols}$
```{r fig.asp=1, fig.width=4}
Beta_ols = solve(t(Xtr) %*% Xtr) %*% t(Xtr) %*% ytr
Beta_ols

y_ols = Xte %*% Beta_ols
xy_range <- range(c(-1.5, 3))
plot(yte, y_ols, xlab="y_te", ylab="y_ols")
abline(lm(y_ols~yte))
# line y=x
ref_range<-seq(-1.6,3,by=0.1)
lines(ref_range,ref_range,
      type="l", lty=2, lwd=2, col="GRAY")
#abline(a=0,b=1,lty=2,lwd=2,col="GRAY")
```

Prediction Error:
```{r}
pred_error = sum((yte - y_ols)^2) / length(yte)
pred_error
```


\prob{
ii. Now obtain the posterior mean $\hat{\boldsymbol{\beta}}_{\text{Bayes}}=E[\beta \mid \mathbf{y}_{\text{tr}}]$ using the g-prior described above and the training data only. Obtain predictions for the test set $\hat{\mathbf{y}}_{\text{Bayes}} = \mathbf{X}_{\text{test}}\hat{\boldsymbol{\beta}}_{\text{Bayes}}$. Plot versus the test data, compute the prediction error, and compare to the OLS prediction error. Explain the results.
}

```{r}
library(ggplot2)
y = ytr
X = Xtr

n = dim(X)[1]
p = dim(X)[2]

g = n
nu0 = 2
s20 = 1

S = 1000

Hg = (g / (g + 1)) * X %*% solve(t(X) %*% X) %*% t(X)
SSRg = t(y) %*% (diag(1, nrow = n) - Hg) %*% y

s2 = 1 / rgamma(S, (nu0 + n) / 2, (nu0 * s20 + SSRg) / 2)
Vb = g * solve(t(X) %*% X) / (g + 1)
Eb = Vb %*% t(X) %*% y

E = matrix(rnorm(S * p, 0, sqrt(s2)), S, p)
beta = t(t(E %*% chol(Vb)) + c(Eb))

beta_bayes = as.matrix(colMeans(beta))

y_bayes = Xte %*% beta_bayes

bayes_df = data.frame(
  observed = yte,
  predicted = y_bayes
)

ggplot(bayes_df, aes(x = observed, y = predicted)) +
  geom_point() +
  geom_smooth(method = 'lm')
```

```{r}
pred_error = sum((yte - y_bayes)^2) / length(yte)
pred_error
```


\prob{
c) Repeat the procedures in b) many times with different randomly generated test and training sets. Compute the average prediction error for both the OLS and Bayesian methods.
}

```{r}
library(dplyr)
N = 100
set.seed(1)
pred_errors = t(sapply(1:N, function(i) {
  y = crime$y
  X = crime %>% select(-y) %>% as.matrix
  train_i = sample.int(length(y), size = round(length(y) / 2), replace = FALSE)
  ytr = y[train_i]
  Xtr = X[train_i, ]
  yte = y[-train_i]
  Xte = X[-train_i, ]
  
  # OLS
  beta_ols = solve(t(Xtr) %*% Xtr) %*% t(Xtr) %*% ytr
  beta_ols
  
  y_ols = Xte %*% beta_ols
  
  pred_error_ols = sum((yte - y_ols)^2) / length(yte)
  
  # Bayes
  y = ytr
  X = Xtr
  
  n = dim(X)[1]
  p = dim(X)[2]
  
  g = n
  nu0 = 2
  s20 = 1
  
  S = 1000
  
  Hg = (g / (g + 1)) * X %*% solve(t(X) %*% X) %*% t(X)
  SSRg = t(y) %*% (diag(1, nrow = n) - Hg) %*% y
  
  s2 = 1 / rgamma(S, (nu0 + n) / 2, (nu0 * s20 + SSRg) / 2)
  Vb = g * solve(t(X) %*% X) / (g + 1)
  Eb = Vb %*% t(X) %*% y
  
  E = matrix(rnorm(S * p, 0, sqrt(s2)), S, p)
  beta = t(t(E %*% chol(Vb)) + c(Eb))
  
  beta_bayes = as.matrix(colMeans(beta))
  
  y_bayes = Xte %*% beta_bayes
  
  pred_error_bayes = sum((yte - y_bayes)^2) / length(yte)

  c(pred_error_ols, pred_error_bayes)
})) %>% as.data.frame
colnames(pred_errors) = c('ols', 'bayes')
```

```{r}
pred_diff = pred_errors %>% transmute(`bayes - ols` = bayes - ols)

ggplot(pred_diff, aes(x = `bayes - ols`)) +
  geom_density() +
  geom_vline(xintercept = 0, lty = 2)
```


# Problem 3: Exercise 10.5

\prob{
Logistic regression variable selection: Consider a logistic regression model for predicting diabetes as a function of $x_1$ = number of pregnancies, $x_2$ = blood pressure, $x_3$ = body mass index, $x_4$ = diabetes pedigree and $x_5$ = age. Using the data in azdiabetes.dat, center and scale each of the x-variables by subtracting the sample average and dividing by the sample standard deviation for each variable. Consider a logistic regression model of the form Pr$(Y_i = 1|\mathbf{x}_i, \boldsymbol{\beta},  \mathbf{z}) = e^{\theta_i}/(1 + e^{\theta_i})$ where


\[
\theta_i = \beta_0+ 
\beta_1\gamma_1x_{i,1} +
\beta_2\gamma_2x_{i,2} +
\beta_3\gamma_3x_{i,3} +
\beta_4\gamma_4x_{i,4} +
\beta_5\gamma_5x_{i,5}
.\] 

In this model, each $\gamma_j$ is either 0 or 1, indicating whether or not variable $j$ is a predictor of diabetes. For example, if it were the case that $\gamma = (1, 1, 0, 0, 0)$, then $\theta_i = \beta_0 + \beta_1x_{i,1}+\beta_2x_{i,2}$. Obtain posterior distributions for $\beta$ and $\gamma$, using independent prior distributions for the parameters, such that $\gamma_j \sim \text{binary}(\frac{1}{2})$, $\beta_0 \sim \text{normal}(0,16)$ and $\beta_j \sim \text{normal}(0,4)$ for each $j > 0$.
}

## Part a

\prob{
a) Implement a Metropolis-Hastings algorithm for approximating the posterior distribution of $\beta$ and $\gamma$. Examine the sequences $\beta_j^{(s)}$ and $\beta_j^{(s)} \times \gamma_j^{(s)}$ for each j and discuss the mixing of the chain.
}

## Part b

\prob{
b) Approximate the posterior probability of the top five most frequently occurring values of $\gamma$. How good do you think the MCMC estimates of these posterior probabilities are?
}

## Part c

\prob{
c) For each $j$, plot posterior densities and obtain posterior means for $\beta_j\gamma_j$. Also obtain Pr$(\gamma_j = 1|\mathbf{x}, \mathbf{y})$.
}

# Problem 4: Exercise 11.4

\prob{
Hierarchical logistic regression: The Washington Assessment of Student Learning (WASL) is a standardized test given to students in the state of Washington. Letting $j$ index the counties within the state of Washington and $i$ index schools within counties, the file mathstandard.dat includes data on the following variables:

$y_{i,j}$ = the indicator that more than half the 10th graders in school $i, j$ passed the WASL math exam;

$x_{i,j}$ = the percentage of teachers in school $i, j$ who have a masters degree.

In this exercise we will construct an algorithm to approximate the posterior distribution of the parameters in a generalized linear mixed-effects model for these data. The model is a mixed effects version of logistic regression:

\begin{align*}
y_{i,j} &\sim \text{binomial}(\frac{e^{\gamma_{i,j}}}{1+e^{\gamma_{i,j}}})\text{, where }\gamma_{i,j} = \beta_{0,j}+\beta_{1,j}x_{i,j}\\
\boldsymbol{\beta}_1,\ldots,\boldsymbol{\beta}_J &\sim \text{ i.i.d. multivariate normal } (\boldsymbol{\theta}, \Sigma)\text{, where } \boldsymbol{\beta}_j = (\beta_{0,j}, \beta_{1,j})
.\end{align*}
}

## Part a

\prob{
a) The unknown parameters in the model include population-level parameters $\{\boldsymbol{\theta}, \Sigma\}$ and the group-level parameters $\{\boldsymbol{\beta}_1, \ldots , \boldsymbol{\beta}_J\}$. Draw a diagram that describes the relationships between these parameters, the data $\{y_{i,j}, x_{i,j}, i=1,\ldots,n_j, j=1,\ldots,J\}$, and prior distributions.
}

\begin{figure}
    \centering
\begin{tikzpicture}
\matrix[matrix of math nodes, column sep=20pt, row sep=20pt] (mat)
{
    &&\left(\boldsymbol{\theta}, \Sigma\right) && \\ 
    \boldsymbol{\beta}_1 & \boldsymbol{\beta}_2 & \ldots & \boldsymbol{\beta}_{J-1} & \boldsymbol{\beta}_J \\
    \mathbf{Y}_1 & \mathbf{Y}_2 & \ldots & \mathbf{Y}_{J-1} &  \mathbf{Y}_J \\
%    \mathbf{\sigma}_1^2 & \mathbf{\sigma}_2^2 & \ldots & \mathbf{\sigma}_{k-1}^2 &  \mathbf{\sigma}_k^2 \\
%    && \boldsymbol{\theta}_2 && \\
};

\foreach \column in {1, 2, 4, 5}
{
    \draw[->,>=latex] (mat-1-3) -- (mat-2-\column);
    \draw[->,>=latex] (mat-2-\column) -- (mat-3-\column);
%    \draw[<-,>=latex] (mat-3-\column) -- (mat-4-\column);
%    \draw[<-,>=latex] (mat-3-\column) -- (mat-4-3);
}
\end{tikzpicture}
\caption{Graphical representation of the hierarchical model.}
    \label{fig:11.4diagram}
\end{figure}


## Part b

\prob{
b) Before we do a Bayesian analysis, we will get some ad hoc estimates of these parameters via maximum likelihood: Fit a separate logistic regression model for each group, possibly using the \FunctionTok{glm} command in R via \NormalTok{beta.j <- glm(y.j \~ X.j, family=binomial)\$coef}. Explain any problems you have with obtaining estimates for each county. Plot $\text{exp}\{\hat{\beta}_{0,j} + \hat{\beta}_{1,j}x\}/(1 + \text{exp}\{\hat{\beta}_{0,j} + \hat{\beta}_{1,j}x\})$ as a function of $x$ for each county and describe what you see. Using maximum likelihood estimates only from those counties with 10 or more schools, obtain ad hoc estimates $\hat{\boldsymbol{\theta}}$ and $\hat{\Sigma}$ of $\boldsymbol{\theta}$ and $\Sigma$. Note that these estimates may not be representative of patterns from schools with small sample sizes.
}

For counties with a small number of schools, the logistic regression tends to have large errors. And when the response variable of sampled data are all zeros or all ones, we can't find a proper regression result.

```{r warning= FALSE}
# Note: Data are preprocessed to replace space in the first column with underscore
mathstandard = as.data.frame(read.table("mathstandard.dat", header=TRUE))

# To see warnings
# options(warn=2, error=recover)
# resume default
# options(warn=0, error=NULL)

par(mfrow = c(3, 4), mar = c(1, 1, 3, 1))
# Create a county list to plot all counties
counties_list <- unique(mathstandard$county)

# plot for each county not using ggplot
for (county in counties_list)
{
county_data = mathstandard[which(mathstandard$county==county),]
model <- glm(county_data$metstandard ~ county_data$percentms, data = county_data, family=binomial)

# Create a sequence of values for the predictor variable
x_seq <- seq(min(county_data$percentms), max(county_data$percentms), length.out = 100)

# Predict probabilities for each value in the sequence
pred_prob <- 1/(1+exp(-model$coef[2]*x_seq-model$coef[1]))

# Plot predicted probabilities against the predictor variable
plot(x_seq, pred_prob, type = "l", ylim = c(0, 1), xlab = "", ylab = "")
points(county_data$percentms, county_data$metstandard)
title(county)
}
```

We have 13 Warning messages when doing glm for each county:

1. glm.fit: fitted probabilities numerically 0 or 1 occurred
2. glm.fit: fitted probabilities numerically 0 or 1 occurred
3. glm.fit: algorithm did not converge
4. glm.fit: fitted probabilities numerically 0 or 1 occurred
5. glm.fit: algorithm did not converge
6. glm.fit: fitted probabilities numerically 0 or 1 occurred
7. glm.fit: fitted probabilities numerically 0 or 1 occurred
8. glm.fit: fitted probabilities numerically 0 or 1 occurred
9. glm.fit: fitted probabilities numerically 0 or 1 occurred
10. glm.fit: fitted probabilities numerically 0 or 1 occurred
11. glm.fit: fitted probabilities numerically 0 or 1 occurred
12. glm.fit: algorithm did not converge
13. glm.fit: fitted probabilities numerically 0 or 1 occurred

These warnings comes from 13 counties that has data points $\leq 3$

```{r}
model <- glm(mathstandard$metstandard ~ mathstandard$percentms, data = mathstandard, family=binomial)

# Create a sequence of values for the predictor variable
x_seq <- seq(min(mathstandard$percentms), max(mathstandard$percentms), length.out = 100)

# Predict probabilities for each value in the sequence
pred_prob <- 1/(1+exp(-model$coef[2]*x_seq-model$coef[1]))

# Plot predicted probabilities against the predictor variable
plot(x_seq, pred_prob, type = "l", ylim = c(0, 1), xlab = "MS Percent", ylab = "Met Standard")
points(mathstandard$percentms, mathstandard$metstandard)
title("The Logistic Regression on the entire Washington State")
```

```{r}
Ncounty <- table(mathstandard$county) # Table for number of sample data in each county
valid_counties <- names(which(Ncounty >= 10))

# Select counties with 10 or more schools

BETA <- NULL
for (county in valid_counties){
county_data = mathstandard[which(mathstandard$county==county),]
model <- glm(county_data$metstandard ~ county_data$percentms, 
	     data = county_data, family=binomial)
BETA<-rbind(BETA, model$coeff)
}

mu0<-thetahat<-apply(BETA, 2 , mean)
S0<-cov(BETA)
```

\[
\hat{\boldsymbol{\theta}} = \begin{pmatrix} \theta_0\\ \theta_1 \end{pmatrix} = \begin{pmatrix} -3.46\\0.05\end{pmatrix}
\] 

\[
\hat{\boldsymbol{\Sigma}} = \begin{bmatrix} 10.756 & -0.179\\-0.1799 & 0.003 \end{bmatrix} 
\] 


\prob{
c) Formulate a unit information prior distribution for $\boldsymbol{ \theta }$ and $\Sigma$ based on the observed data. Specifically, let $\boldsymbol{ \theta } \sim \text{multivariate normal}(\hat{\boldsymbol{\theta}}, \hat{\Sigma})$ and let $\Sigma^{-1} \sim \text{Wishart}(4, \hat{\Sigma}^{-1})$. Use a Metropolis-Hastings algorithm to approximate the joint posterior distribution of all parameters.
}

Our unit information prior of $\boldsymbol{\theta}$ is that it follows a multivariate normal distribution MVN $(\boldsymbol{\mu}, \boldsymbol{\Lambda}_0)$  with an expectation of $\boldsymbol{\mu} = \hat{\boldsymbol{\theta}} = \frac{1}{12}\sum_{j=1}^{12} \tilde{\boldsymbol{\beta}_j}$ and a prior covariance matrix $\boldsymbol{\Lambda}_0$ equal to $\hat{\boldsymbol{\Sigma}}$, the samples covariance matrix of $\tilde{\boldsymbol{\beta}_j}$. 

Our unit information prior of $\boldsymbol{\Sigma}$ is that it follows a inverse-Wishart distribution $(\eta_0, \boldsymbol{S}_0)$.  $\eta_0 = p+2 = 4$ and $\boldsymbol{S}_0$ is also set equal to $\hat{\boldsymbol{\Sigma}}$. 

Given $\Sigma$ and our sample of regression coefficients $\boldsymbol{\beta}_1, \ldots, \boldsymbol{\beta}_J$, the full conditional distribution of $\boldsymbol{\theta}$ is as follows:

\begin{align*}
\{\boldsymbol{\theta}|\boldsymbol{\beta}_1, \ldots, \boldsymbol{\beta}_J, \Sigma\} &\sim \text{multivariate normal}(\boldsymbol{\mu}_J, \Lambda_J)\text{, where}\\
\Lambda_J &= (\Lambda_0^{-1}+J\Sigma^{-1})^{-1} \\
\boldsymbol{\mu}_J &= \Lambda_J(\Lambda_0^{-1}\boldsymbol{\mu}_0+J\Sigma^{-1}\overline{\boldsymbol{\beta}}) \\
\end{align*}

where $\overline{\boldsymbol{\beta}}$ is the vector average $\frac{1}{J}\sum_{j=1}^{J} \boldsymbol{\beta}_j$.

The full conditional distribution of a covariance matrix is an inverse-Wishart distribution, with sum of squares matrix equal to the prior sum of sqares $\mathbf{S}_0$ plus the sum of squares from the sample:

\begin{align*}
\{\Sigma|\boldsymbol{\theta},\boldsymbol{\beta}_1, \ldots, \boldsymbol{\beta}_J\} &\sim \text{inverse-Wishart}(\eta_0+J, [\mathbf{S}_0+\mathbf{S}_\theta]^{-1})\text{, where}\\
\mathbf{S}_\theta &= \sum_{j=1}^{J} (\boldsymbol{\beta}_j-\boldsymbol{\theta})(\boldsymbol{\beta}_j-\boldsymbol{\theta})^{T} \\
.\end{align*}

For sampling $\boldsymbol{\beta}$, we follow these steps: 

![Sampling Beta using Metropolis-Hastings Algorithm (Sorry for the format...)](sampling-beta.png)

```{r eval=FALSE}
p<-2 # 1, percentms
J<-m<-39 # J and m are used interchangeably
eta0 <- p+2
# inverse Lambda_0, inverse S_0
iL0<-iSigma<-solve(S0)

# Recalculate BETA, the ith row represent beta for the ith county
# At the same time, prepare the data
BETA <- NULL
i<-1
Y<-data.frame(nrow=J)
X<-data.frame(nrow=J)
for (county in counties_list){
county_data = mathstandard[which(mathstandard$county==county),]
if (nrow(county_data) <= 10){
	BETA <- rbind(BETA, c(0,0))
}else{
	model <- glm(county_data$metstandard ~ county_data$percentms, 
		     data = county_data, family=binomial)
	BETA<-rbind(BETA, model$coeff)
}
Y[i] <- t(county_data$metstandard)
X[i] <- t(county_data$percentms)
i<-i+1
}

# Utility function for sampling from binomial p(y|X,beta)
# to generate beta using Metropolis-Hastings Algorithm
XBeta <- function(X,Beta){
# X is a vector, Beta is a vector c(intercept, slop)
return(Beta[1]+Beta[2]*X)
}
sigmoid <- function(x){
	return(1/(1+exp(-x)))
}
## mvnormal simulation
rmvnorm<-function(n,mu,Sigma)
{ 
  E<-matrix(rnorm(n*length(mu)),n,length(mu))
  t(  t(E%*%chol(Sigma)) +c(mu))
}

## Wishart simulation
rwish<-function(n,nu0,S0)
{
  sS0 <- chol(S0)
  S<-array( dim=c( dim(S0),n ) )
  for(i in 1:n)
  {
     Z <- matrix(rnorm(nu0 * dim(S0)[1]), nu0, dim(S0)[1]) %*% sS0
     S[,,i]<- t(Z)%*%Z
  }
  S[,,1:n]
}
## mvnorm log density
ldmvnorm<-function(X,mu,Sigma,iSigma=solve(Sigma),dSigma=det(Sigma))
{
  Y<-t( t(X)-mu)
  sum(diag(-.5*t(Y)%*%Y%*%iSigma))  -
  .5*(  prod(dim(X))*log(2*pi) +     dim(X)[1]*log(dSigma) )
}


## MCMC
ACCEPT.count <- rep(1,m)
THETA.post<-SIGMA.post<-NULL ; set.seed(1)
for(s in 1:500000) 
{

  ##update theta
  Lm<-solve( iL0 +  m*iSigma )
  mum<-Lm%*%( iL0%*%mu0 + iSigma%*%apply(BETA,2,sum) )
  theta<-t(rmvnorm(1,mum,Lm))

  ##update Sigma
  mtheta<-matrix(theta,m,p,byrow=TRUE)
  iSigma<-rwish(1,eta0+m, 
           solve( S0+t(BETA-mtheta)%*%(BETA-mtheta)) )

  ##update beta
  Sigma<-solve(iSigma) ; dSigma<-det(Sigma)
  for(j in 1:m)
  {
    # beta proposed (beta^(*)) from multivariate normal (beta_j^(s), V_J^(s)
    beta.p<-t(rmvnorm(1,BETA[j,],.5*Sigma))
########## Test dbinom ########
#yy <- as.vector(t(Y[1]))
#xx <- as.vector(t(X[1]))
#sigmoid(XBeta(xx,BETA[1,]))
#dbinom(yy, size=1, prob=sigmoid(XBeta(xx,BETA[1,])), log=TRUE)
#dbinom(Y[j,],size=1, prob=exp(X%*%beta.p),log=TRUE )
########## Test dbinom ########
    lr<-sum( 
	    dbinom(as.vector(t(Y[j])),size=1, 
		   prob=sigmoid(XBeta(as.vector(t(X[j])),beta.p)),log=TRUE ) -
	    dbinom(as.vector(t(Y[j])),size=1, 
		   prob=sigmoid(XBeta(as.vector(t(X[j])),BETA[j, ])),log=TRUE ) ) +
        ldmvnorm( t(beta.p),theta,Sigma,
              iSigma=iSigma,dSigma=dSigma ) -
        ldmvnorm( t(BETA[j,]),theta,Sigma,
              iSigma=iSigma,dSigma=dSigma )

    if( log(runif(1))<lr ) { 
	    ACCEPT.count[j] <- ACCEPT.count[j]+1
	    BETA[j,]<-beta.p }
   }

  ##store some output
  if(s%%350==0) # saving every 350th value
  {  
    cat(s,"\n") 
    THETA.post<-rbind(THETA.post,t(theta)) 
    SIGMA.post<-rbind(SIGMA.post,c(Sigma)) 
  }

}
```

```{r eval=FALSE}
save(THETA.post, file="THETA.post.RData")
save(SIGMA.post, file="SIGMA.post.RData")
save(ACCEPT.count, file="ACCEPT.count.RData")
save(BETA, file="BETA.RData")
```

```{r}
load("THETA.post.RData")
load("SIGMA.post.RData")
load("ACCEPT.count.RData")
load("BETA.RData")
```

## Part d

\prob{
d) Make plots of the samples of $\boldsymbol{\theta}$ and $\Sigma$ (5 parameters) versus MCMC iteration number. Make sure you run the chain long enough so that your MCMC samples are likely to be a reasonable approximation to the posterior distribution.
}

```{r}
## MCMC diagnostics
library(coda)
cat("The effective sample size of THETA")
round(apply(THETA.post,2,effectiveSize),2)
cat("The effective sample size of SIGMA")
round(apply(SIGMA.post,2,effectiveSize),2)
```

Assess the convergence of the Markov chain
```{r}
stationarity.plot(THETA.post[,1],xlab="iteration",ylab=expression(theta[0]))
stationarity.plot(THETA.post[,2],xlab="iteration",ylab=expression(theta[1]))
stationarity.plot(SIGMA.post[,1],xlab="iteration",ylab=expression(sigma[theta[0]]))
stationarity.plot(SIGMA.post[,2],xlab="iteration",ylab=expression(sigma[theta[0],theta[1]]))
stationarity.plot(SIGMA.post[,4],xlab="iteration",ylab=expression(sigma[theta[1]]))
```

```{r}
acf(THETA.post[,1],ci.col="gray",xlab="lag", main = 'autocorrelation function (THETA_0)')
acf(THETA.post[,2],ci.col="gray",xlab="lag", main = 'autocorrelation function (THETA_1)')
acf(SIGMA.post[,1],ci.col="gray",xlab="lag", main = 'autocorrelation function (SIGMA_THETA_0^2)')
acf(SIGMA.post[,2],ci.col="gray",xlab="lag", main = 'autocorrelation function (SIGMA_THETA_0_THETA_1)')
acf(SIGMA.post[,4],ci.col="gray",xlab="lag", main = 'autocorrelation function (SIGMA_THETA_1^2)')
```


Assess the acceptance rate of Metropolis-Hastings Algorithm:

The acceptance rate when sampling $\boldsymbol{\beta}_j$ is plotted in this figure. The two dashed lines here are $r_1 = 0.3$ and $r_2 = 0.7$, we can see from this graph that our acceptance rate is a little bit high, but still acceptable.

```{r}
plot(ACCEPT.count/500000, ylim=c(0,1))
abline(h=0.3,lty=2,lwd=2)
abline(h=0.7,lty=2,lwd=2)
```

## Part e

\prob{
e) Obtain posterior expectations of $\boldsymbol{\beta}_j$ for each group $j$, plot $E[\beta_{0,j}|\mathbf{y}] + E[\beta_{1,j}|\mathbf{y}]x$ as a function of $x$ for each county, compare to the plot in b) and describe why you see any differences between the two sets of regression lines.
}

```{r}
```



## Part f

\prob{
f) From your posterior samples, plot marginal posterior and prior densities of $\boldsymbol{\theta}$ and the elements of $\Sigma$. Include your ad hoc estimates from b) in the plots. Discuss the evidence that the slopes or intercepts vary across groups.
}
